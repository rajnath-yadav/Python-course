{"cells":[{"cell_type":"markdown","source":["### Introduction to Word Embeddings  \n\nMany people would say the breakthrough of deep learning in natural language processing started with the introduction of word embeddings. Rather than using the words themselves as features, neural network methods typically take as input dense, relatively low-dimensional vectors that model the meaning and usage of a word. Word embeddings were first popularized through the [Word2Vec](https://arxiv.org/abs/1301.3781) model, developed by Thomas Mikolov and colleagues at Google. Since then, scores of alternative approaches have been developed, such as [GloVe](https://nlp.stanford.edu/projects/glove/) and [FastText](https://fasttext.cc/) embeddings. In this notebook, we'll explore word embeddings with the original Word2Vec approach, as implemented in the [Gensim](https://radimrehurek.com/gensim/) library."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1bd13a3d-94b7-4398-82fe-416d090e40b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!pip install -U -q gensim\n!pip install -U -q spacy"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f75d86c2-4135-4a9f-82c5-65077d79a9ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport pandas as pd\n%matplotlib inline\n\nimport os\nimport csv\nimport spacy\nimport gensim\nimport time\n\n%matplotlib inline\n\nfrom sklearn.manifold import TSNE"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe10d051-ea80-4128-8ac5-46f2e1886315","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Disabling MLFlow autologging"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d949c9e8-4754-46c5-8f9c-19e7cb0edd75","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import mlflow\nmlflow.autolog(disable=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8706f70d-7814-48fd-b59f-0a856fdee36a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Training word embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b22ca4ad-1857-4b69-a486-bba1058de3b0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Training word embeddings with Gensim couldn't be easier. The only thing we need is a corpus of sentences in the language of interest.  \nFor our experiments we're going to use the abstracts of all ArXiv papers in the category cs.CL (computation and language) that were published before mid-April 2021 â€” a total of around 25,000 documents.  \nWe tokenize these abstracts with [_spaCy_](https://spacy.io/)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05b18815-3caf-4d49-b868-c4264ad16ce3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["!head -10 data/arxiv/arxiv.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28849b70-6227-4fd3-9adf-ef521515eaa2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class Corpus(object):\n    \n    def __init__(self, filename):\n        self.filename = filename\n        self.nlp = spacy.blank(\"en\")\n        \n    def __iter__(self):\n        with open(self.filename, \"r\") as i:\n            reader = csv.reader(i, delimiter=\",\")\n            for _, abstract in reader:\n                tokens = [t.text.lower() for t in self.nlp(abstract)]\n                yield tokens\n                            \n                    \ndocuments = Corpus(os.path.join(os.getcwd(), \"data/arxiv/arxiv.csv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbe66a00-58a3-46d3-8689-012912b7aa28","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["list(documents)[0:2]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b7a2f9a-3f49-414f-887f-24ce920c381e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When we train our word embeddings, Gensim allows us to set a number of parameters. The most important of these are `min_count`, `window`, `vector_size` and `sg`:\n\n- `min_count` is the minimum frequency of the words in our corpus. For infrequent words we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more.\n- `window` is the number of words to the left and to the right that make up the context that word2vec will take into account.\n- `vector_size` is the dimensionality of the word vectors. This is generally between 100 and 1000. This dimensionality often forces us to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n- `sg`: there are two algorithms to train `Word2Vec`: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (`sg=0`).\n\nWe'll investigate the impact of some of these parameters later."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e88d6667-7852-46d7-aba6-e149453cda6b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from gensim import corpora\n\ndictionary = corpora.Dictionary(documents)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c587a293-b796-40d7-b474-8c9fd06bd750","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(dictionary.token2id)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63154e8f-5106-4683-9e1e-88d7c3f08870","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# https://radimrehurek.com/gensim/models/word2vec.html\n\nmodel = gensim.models.Word2Vec(documents, min_count=100, window=5, vector_size=100)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbbb1a02-d527-449c-af8a-38bbbb1fa303","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Using word embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0835cc44-b4a7-4d11-8ac5-feb8eba9e419","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let's take a look at the trained model. The word embeddings are on its `wv` attribute and we can access them by  using the token as key. For example, here is the embedding for *nlp*, with the requested 100 dimensions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5149430-90f3-40df-8c30-136beb7ad322","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(model.wv[\"nlp\"].shape)\nmodel.wv[\"nlp\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4118c593-4564-4c5b-b159-4884b3293222","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can also easily find the similarity between two words. Similarity is measured as the cosine between the two word embeddings, and therefore ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that *nmt* (neural machine translation) is closer to *smt* (statistical machine translation) than to *ner* (named entity recognition)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2ced558-0696-48ed-97d7-42f69b82ab80","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(model.wv.similarity(\"nmt\", \"smt\"))\nprint(model.wv.similarity(\"nmt\", \"ner\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e4fee11-38e1-413a-aa1c-6cc6d459cbd1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to *bert* are all semantically related to it: other types of pretrained models such as *roberta*, *mbert*, *xlm*, as well as the more general model type BERT represents (*transformer* and *transformers*), and more generally related words (*pretrained*)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d326d2b-99de-4b48-ae81-0a2dd62cc0c3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["model.wv.similar_by_word(\"bert\", topn=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"602180f0-97f4-49ee-a4cb-f4e8aa8011b0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type *\"BERT is to a transformer like an LSTM is to ...\"*. Our embedding model correctly predicts that LSTMs are a type of RNN, just like BERT is a particular type of transformer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d014d848-9852-402a-9260-af993947a6e4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["model.wv.most_similar(positive=[\"transformer\", \"lstm\"], negative=[\"bert\"], topn=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d73d221b-51cd-4502-bba8-549c4941d45f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Similarly, we can also zoom in on one of the meanings of ambiguous words. For example, in NLP *tree* has a very specific meaning, which is obvious from its nearest neighbours *constituency*, *parse*, *dependency* and *syntax*."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73ea1baa-834c-4823-b5bc-04a3bda152ff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["model.wv.most_similar(positive=[\"tree\"], topn=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93c924ed-f204-4278-93f3-3177d7abb7a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["However, if we specify we're looking for words that are similar to *tree*, but dissimilar to *syntax*, suddenly its standard meaning takes over, and *forest* crops up in its nearest neighbours."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f29d5fd-c98a-4cbf-ad70-1113b04dba32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["model.wv.most_similar(positive=[\"tree\"], negative=[\"syntax\"], topn=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53de6be9-ee10-4702-9e15-13bafa745366","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, we can present the `WordVec` model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list *lstm cnn gru svm transformer*, it correctly identifies *svm* as the only non-neural model. In the list *bert word2vec gpt-2 roberta xlnet*, it correctly singles out *word2vec* as the only non-transormer model. In *word2vec bert glove fasttext elmo*, *bert* is singled out as the only transformer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1c820ff-c994-46c1-87bb-71278aa2c890","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(model.wv.doesnt_match(\"lstm cnn gru svm transformer\".split()))\nprint(model.wv.doesnt_match(\"bert word2vec gpt-2 roberta xlnet\".split()))\nprint(model.wv.doesnt_match(\"word2vec bert glove fasttext elmo\".split()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0da66f81-61cb-4a20-8b52-1fa30ae8b751","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Plotting embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c7889bb-9e81-4413-847a-cb66dfadc35d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let's now visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular [t-SNE](https://lvdmaaten.github.io/tsne/) method. T-SNE, short for **t-distributed Stochastic Neighbor Embedding**, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distant points in the low-dimensional space.\n\nT-SNE is present in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). To run it, we just have to specify the number of dimensions we'd like to map the data to (`n_components`), and the similarity metric that t-SNE should use to compute the similarity between two data points (`metric`). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The [Scikit-learn user guide](https://scikit-learn.org/stable/modules/manifold.html#t-sne) contains some additional tips for optimizing performance. \n\nPlotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5bc04b4f-4312-432a-9379-c8dd3682ff69","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["target_word = \"bert\"\n\nselected_words = [w[0] for w in model.wv.most_similar(positive=[target_word], topn=200)] + [target_word]\nselected_words"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c20a2491-ae04-4d47-af1c-e6f90db71ab5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If we take *bert* as our target word, the figure shows some interesting patterns. In the immediate vicinity of *bert*, we find the similar transformer models that we already identified as its nearest neighbours earlier: *xlm*, *mbert*, *gpt-2*, and so on. Other parts of the picture have equally informative clusters of NLP tasks and benchmarks (*squad* and *glue*), languages (*german* and *english*), neural-network architectures (*lstm*, *gru*, etc.), embedding types (*word2vec*, *glove*, *fasttext*, *elmo*), etc."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6385bbe-417e-4e64-af43-3cc2819e405e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["embeddings = [model.wv[w] for w in selected_words] + model.wv[\"bert\"]\nmapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca', square_distances=True).fit_transform(embeddings)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"011559bc-b2f3-4cb4-8380-53dd81673be9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plt.figure(figsize=(20,20))\nx = mapped_embeddings[:,0]\ny = mapped_embeddings[:,1]\nplt.scatter(x, y)\n\nfor i, txt in enumerate(selected_words):\n    plt.annotate(txt, (x[i], y[i]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32eb9c8e-f9ef-475f-a879-bd516186865a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### From word vectors to document vectors\n\nhttps://radimrehurek.com/gensim/models/doc2vec.html"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3332824b-6c3d-446b-8108-529b1076afac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from gensim.models.doc2vec import Doc2Vec, TaggedDocument"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c31dbf65-bdeb-4663-8097-ba1c2dd4c315","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["documents2 = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]\nmodel2 = Doc2Vec(documents2, vector_size=100, window=5, min_count=1, workers=4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bd8fedb-9559-4689-8925-9a53abe3c4da","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model2.dv[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfb52888-0d29-4a25-ad7e-c0dcc52a834a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["docvector = model2.infer_vector(['this','is','a','new','example','document'])\ndocvector"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3af250ab-47cb-4e4c-aac3-1acd6487383a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Conclusions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"347ce715-ec41-4775-ba62-43896271eea7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Word embeddings were one of the big trends on Natural Language Processing a decade ago. They allow us to model the meaning and usage of a word, and discover words that behave similarly. This is crucial for the generalization capacity of many machine learning models. Moving from raw strings to embeddings allows them to generalize across words that have a similar meaning, and to discover patterns that had previously escaped them."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ce91206-e2a2-419b-ac64-506af5c18236","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02_NLP_Word_Embeddings","dashboards":[],"notebookMetadata":{},"language":"python","widgets":{},"notebookOrigID":3680572085114829}},"nbformat":4,"nbformat_minor":0}
